# uncomment when calling script from debugger
# seed: 1234
# dataset: json2vec-car
cross_val: none
repeat: no

model:
  # number of transformer blocks
  n_layer: 4

  # number of attention heads
  n_head: 4

  # embedding dimensionality (must be a multiple of n_head)
  n_embd: 128

  # whether to re-use the embedding matrix for the output layer
  tie_weights: no

  # position encoding: NONE, INTEGER, KEY_VALUE
  position_encoding: KEY_VALUE

  # whether to fuse the position embedding with an MLP or just add to token embedding
  fuse_pos_with_mlp: no

  # whether to mask field token losses in training
  mask_field_token_losses: no

  # guardrails mode: NONE, STRUCTURE_ONLY, STRUCTURE_AND_VALUES
  guardrails: STRUCTURE_AND_VALUES

train:
  # device to run on: cpu, cuda, mps, auto
  device: auto

  # bach size to train
  batch_size: 100

  # number of batches total to train
  n_batches: 10000

  # how many warmup batches (with slowly increasing lr)
  n_warmup_batches: 1000

  # max learning rate (decays linearly after)
  learning_rate: 1e-3

  # end factor of learning rate at end of training (e.g. 1%)
  lr_end_factor: 0.01

  # split ratio for train/test set if cross-validation is disabled  (e.g. 0.2 => 80% train, 20% test)
  test_split: 0.2

  # whether to shuffle rows before splitting
  shuffle_split: yes

  # after how many batches to print loss information
  print_every: 10

  # after how many batches to run an evaluation
  eval_every: 100

  # how many samples of the train data to evaluate on (0 = disabled)
  sample_train: 100

  # how many samples of the test data to evaluate on (0 = disabled)
  sample_test: 100

pipeline:
  # set to positive integer to limit the number of tokens in the vocabulary
  max_vocab_size: 0

  # how to handle high-cardinality numeric values (NONE, BINNING)
  numeric_method: BINNING

  # how many bins to use for numeric value binning
  n_bins: 100

  # sequence order of tokens, ORDERED or SHUFFLED
  sequence_order: SHUFFLED

  # by how much to upscale the data (only makes sense for SHUFFLED)
  upscale: 4

  # whether to encode the full field path or just the field name
  # if no, VPDA cannot be used with schema
  path_in_field_tokens: yes
