{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an ORiGAMi model on Tabular Data from OpenML\n",
    "\n",
    "This notebook demonstrates how to train an ORiGAMi model on tabular data from [OpenML](https://www.openml.org/).\n",
    "\n",
    "While ORiGAMi is designed for semi-structured data, it can also be applied to tabular data, which is a special case of semi-structured data where all objects only have top-level fields and no missing fields.\n",
    "\n",
    "The notebook trains a model on the [Tic Tac Toe dataset](https://www.openml.org/search?type=data&status=active&id=50), a dataset that encodes all possible board configurations at the end of tic tac toe games. The target label to predict is whether the player who plays \"x\" wins or loses. This is not a difficult dataset and many classic algorithms can achieve 100% classification accuracy. We use this relatively small dataset to show how the data and model are prepared for training.\n",
    "\n",
    "Note that if you choose a different OpenML dataset, or even your own, the hyperparameters and model configuration may not be ideal and you might have to do some hyperparameter exploration to get the best results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "OpenML datasets have unique IDs. The Tic Tac Toe dataset has ID 50. For other datasets, see the [Datasets](https://www.openml.org/search?type=data&status=active) page on OpenML's website.\n",
    "\n",
    "First we load the data into a list of dictionaries and also get the target field name from the metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top-left-square</th>\n",
       "      <th>top-middle-square</th>\n",
       "      <th>top-right-square</th>\n",
       "      <th>middle-left-square</th>\n",
       "      <th>middle-middle-square</th>\n",
       "      <th>middle-right-square</th>\n",
       "      <th>bottom-left-square</th>\n",
       "      <th>bottom-middle-square</th>\n",
       "      <th>bottom-right-square</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top-left-square top-middle-square top-right-square middle-left-square  \\\n",
       "0               x                 x                x                  x   \n",
       "1               x                 x                x                  x   \n",
       "2               x                 x                x                  x   \n",
       "3               x                 x                x                  x   \n",
       "4               x                 x                x                  x   \n",
       "\n",
       "  middle-middle-square middle-right-square bottom-left-square  \\\n",
       "0                    o                   o                  x   \n",
       "1                    o                   o                  o   \n",
       "2                    o                   o                  o   \n",
       "3                    o                   o                  o   \n",
       "4                    o                   o                  b   \n",
       "\n",
       "  bottom-middle-square bottom-right-square     Class  \n",
       "0                    o                   o  positive  \n",
       "1                    x                   o  positive  \n",
       "2                    o                   x  positive  \n",
       "3                    b                   b  positive  \n",
       "4                    o                   b  positive  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 958\n",
      "Target field for classification: \"Class\"\n",
      "Example instance:\n",
      "{'top-left-square': 'x', 'top-middle-square': 'x', 'top-right-square': 'x', 'middle-left-square': 'x', 'middle-middle-square': 'o', 'middle-right-square': 'o', 'bottom-left-square': 'x', 'bottom-middle-square': 'o', 'bottom-right-square': 'o', 'Class': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "from openml import datasets\n",
    "\n",
    "# Fetch tic tac toe dataset by ID (50)\n",
    "dataset = datasets.get_dataset(50)\n",
    "\n",
    "# Get the data in pandas DataFrame format\n",
    "X, _, _, _ = dataset.get_data()\n",
    "\n",
    "display(X.head())\n",
    "\n",
    "# Convert DataFrame to list of dictionaries\n",
    "data = X.to_dict(\"records\")\n",
    "\n",
    "# get the name of the target label\n",
    "target_field = dataset.default_target_attribute\n",
    "\n",
    "print(f\"Number of instances: {len(data)}\")\n",
    "print(f'Target field for classification: \"{target_field}\"')\n",
    "print(f\"Example instance:\\n{data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Pipeline\n",
    "\n",
    "Next, we split the data in train and test set and create a prediction pipeline with the built-in `build_prediction_pipelines()` utility.\n",
    "\n",
    "We also get access to the `schema`, `encoder` and `block_size` variable which we'll need for model creation.\n",
    "\n",
    "While most of the datasets in the ORiGAMi paper use a _shuffled_ training approach with _data upscaling_, in the case of Tic Tac Toe, we don't need this, because the individual field positions are not causally related to each other.\n",
    "\n",
    "For other datasets, you may get better results with shuffling and upscaling:\n",
    "\n",
    "```python\n",
    "# pipeline config\n",
    "config.pipeline.upscale = 10 # you can experiment with different values here\n",
    "config.pipeline.sequence_order = \"SHUFFLED\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pipeline: Pipeline(steps=[('binning',\n",
      "                 KBinsDiscretizerPipe(strategy='kmeans', threshold=100)),\n",
      "                ('target', TargetFieldPipe(target_field='Class')),\n",
      "                ('schema', SchemaParserPipe()),\n",
      "                ('tokenizer', DocTokenizerPipe()),\n",
      "                ('padding', PadTruncTokensPipe()),\n",
      "                ('encoder', TokenEncoderPipe(max_tokens=0))],\n",
      "         verbose=True)\n",
      "test pipeline: Pipeline(steps=[('binning',\n",
      "                 KBinsDiscretizerPipe(strategy='kmeans', threshold=100)),\n",
      "                ('target', TargetFieldPipe(target_field='Class')),\n",
      "                ('tokenizer', DocTokenizerPipe()),\n",
      "                ('padding', PadTruncTokensPipe()),\n",
      "                ('encoder', TokenEncoderPipe(max_tokens=0))],\n",
      "         verbose=True)\n",
      "[Pipeline] ........... (step 1 of 6) Processing binning, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 6) Processing target, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 6) Processing schema, total=   0.0s\n",
      "[Pipeline] ......... (step 4 of 6) Processing tokenizer, total=   0.0s\n",
      "[Pipeline] ........... (step 5 of 6) Processing padding, total=   0.0s\n",
      "[Pipeline] ........... (step 6 of 6) Processing encoder, total=   0.0s\n",
      "len train: 766, len test: 192\n",
      "vocab size 25\n",
      "block size 23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from origami.preprocessing import build_prediction_pipelines, docs_to_df\n",
    "from origami.utils import set_seed\n",
    "from origami.utils.config import TopLevelConfig\n",
    "\n",
    "# for reproducibility\n",
    "set_seed(123)\n",
    "\n",
    "# load data into \"docs\" column in dataframe and split into train/test\n",
    "df = docs_to_df(data)\n",
    "train_docs_df, test_docs_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "config = TopLevelConfig()\n",
    "\n",
    "# pipeline config\n",
    "config.pipeline.upscale = 1\n",
    "config.pipeline.sequence_order = \"ORDERED\"\n",
    "\n",
    "# create train and test pipelines\n",
    "pipelines = build_prediction_pipelines(pipeline_config=config.pipeline, target_field=target_field, verbose=True)\n",
    "\n",
    "# process train and test data\n",
    "train_df = pipelines[\"train\"].fit_transform(train_docs_df)\n",
    "test_df = pipelines[\"test\"].transform(test_docs_df)\n",
    "\n",
    "# get stateful objects\n",
    "schema = pipelines[\"train\"][\"schema\"].schema\n",
    "encoder = pipelines[\"train\"][\"encoder\"].encoder\n",
    "block_size = pipelines[\"train\"][\"padding\"].length\n",
    "\n",
    "# print data stats\n",
    "print(f\"len train: {len(train_df)}, len test: {len(test_df)}\")\n",
    "print(f\"vocab size {encoder.vocab_size}\")\n",
    "print(f\"block size {block_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "\n",
    "Now we configure the model and create a model instance and a pushdown automaton (VPDA = Vectorised Pushdown Automaton) which we'll pass to the model.\n",
    "\n",
    "We pass in the encoder and schema to the VPDA class, which will automatically create transition rules that only allow next tokens that lead to valid objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 0.20M\n"
     ]
    }
   ],
   "source": [
    "from origami.model import ORIGAMI\n",
    "from origami.model.vpda import ObjectVPDA\n",
    "from origami.preprocessing import DFDataset\n",
    "from origami.utils import count_parameters\n",
    "\n",
    "# wrap dataframes in datasets\n",
    "train_dataset = DFDataset(train_df)\n",
    "test_dataset = DFDataset(test_df)\n",
    "\n",
    "# model config\n",
    "config.model.n_layer = 4\n",
    "config.model.n_head = 4\n",
    "config.model.n_embd = 64\n",
    "config.model.vocab_size = encoder.vocab_size\n",
    "config.model.block_size = block_size\n",
    "\n",
    "# create PDA and pass it to the model\n",
    "vpda = ObjectVPDA(encoder, schema)\n",
    "model = ORIGAMI(config.model, config.train, vpda=vpda)\n",
    "\n",
    "n_params = count_parameters(model)\n",
    "print(f\"Number of parameters: {n_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "We configure the training parameters, create a `Predictor` instance for evaluation, a progress callback function, and train the model for 2000 steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  step: 0  |  epoch: 0  |  batch_num: 0  |  batch_dt: 0.00  |  batch_loss: 2.2953  |  lr: 1.01e-06  |  train_acc: 0.6300  |  test_loss: 2.2927  |  test_acc: 0.6500  |\n",
      "|  step: 1  |  epoch: 1  |  batch_num: 10  |  batch_dt: 47.73  |  batch_loss: 2.2590  |  lr: 1.10e-05  |\n",
      "|  step: 2  |  epoch: 2  |  batch_num: 20  |  batch_dt: 49.26  |  batch_loss: 2.2035  |  lr: 2.10e-05  |\n",
      "|  step: 3  |  epoch: 3  |  batch_num: 30  |  batch_dt: 47.07  |  batch_loss: 2.1020  |  lr: 3.10e-05  |\n",
      "|  step: 4  |  epoch: 5  |  batch_num: 40  |  batch_dt: 50.02  |  batch_loss: 1.9753  |  lr: 4.10e-05  |\n",
      "|  step: 5  |  epoch: 6  |  batch_num: 50  |  batch_dt: 47.68  |  batch_loss: 1.8567  |  lr: 5.10e-05  |\n",
      "|  step: 6  |  epoch: 7  |  batch_num: 60  |  batch_dt: 51.26  |  batch_loss: 1.7385  |  lr: 6.10e-05  |\n",
      "|  step: 7  |  epoch: 8  |  batch_num: 70  |  batch_dt: 48.23  |  batch_loss: 1.6089  |  lr: 7.10e-05  |\n",
      "|  step: 8  |  epoch: 10  |  batch_num: 80  |  batch_dt: 49.82  |  batch_loss: 1.4764  |  lr: 8.10e-05  |\n",
      "|  step: 9  |  epoch: 11  |  batch_num: 90  |  batch_dt: 47.05  |  batch_loss: 1.3195  |  lr: 9.10e-05  |\n",
      "|  step: 10  |  epoch: 12  |  batch_num: 100  |  batch_dt: 51.64  |  batch_loss: 1.1550  |  lr: 1.01e-04  |  train_acc: 0.6600  |  test_loss: 1.1102  |  test_acc: 0.5900  |\n",
      "|  step: 11  |  epoch: 13  |  batch_num: 110  |  batch_dt: 61.03  |  batch_loss: 1.0209  |  lr: 1.11e-04  |\n",
      "|  step: 12  |  epoch: 15  |  batch_num: 120  |  batch_dt: 49.37  |  batch_loss: 0.8945  |  lr: 1.21e-04  |\n",
      "|  step: 13  |  epoch: 16  |  batch_num: 130  |  batch_dt: 47.49  |  batch_loss: 0.7908  |  lr: 1.31e-04  |\n",
      "|  step: 14  |  epoch: 17  |  batch_num: 140  |  batch_dt: 53.16  |  batch_loss: 0.7128  |  lr: 1.41e-04  |\n",
      "|  step: 15  |  epoch: 18  |  batch_num: 150  |  batch_dt: 49.83  |  batch_loss: 0.6602  |  lr: 1.51e-04  |\n",
      "|  step: 16  |  epoch: 20  |  batch_num: 160  |  batch_dt: 48.22  |  batch_loss: 0.6193  |  lr: 1.61e-04  |\n",
      "|  step: 17  |  epoch: 21  |  batch_num: 170  |  batch_dt: 45.73  |  batch_loss: 0.5864  |  lr: 1.71e-04  |\n",
      "|  step: 18  |  epoch: 22  |  batch_num: 180  |  batch_dt: 46.89  |  batch_loss: 0.5664  |  lr: 1.81e-04  |\n",
      "|  step: 19  |  epoch: 23  |  batch_num: 190  |  batch_dt: 52.66  |  batch_loss: 0.5445  |  lr: 1.91e-04  |\n",
      "|  step: 20  |  epoch: 25  |  batch_num: 200  |  batch_dt: 69.42  |  batch_loss: 0.5324  |  lr: 2.01e-04  |  train_acc: 0.7100  |  test_loss: 0.5197  |  test_acc: 0.6700  |\n",
      "|  step: 21  |  epoch: 26  |  batch_num: 210  |  batch_dt: 48.72  |  batch_loss: 0.5258  |  lr: 2.11e-04  |\n",
      "|  step: 22  |  epoch: 27  |  batch_num: 220  |  batch_dt: 46.19  |  batch_loss: 0.5174  |  lr: 2.21e-04  |\n",
      "|  step: 23  |  epoch: 28  |  batch_num: 230  |  batch_dt: 50.61  |  batch_loss: 0.5036  |  lr: 2.31e-04  |\n",
      "|  step: 24  |  epoch: 30  |  batch_num: 240  |  batch_dt: 53.14  |  batch_loss: 0.4943  |  lr: 2.41e-04  |\n",
      "|  step: 25  |  epoch: 31  |  batch_num: 250  |  batch_dt: 47.35  |  batch_loss: 0.4844  |  lr: 2.51e-04  |\n",
      "|  step: 26  |  epoch: 32  |  batch_num: 260  |  batch_dt: 51.67  |  batch_loss: 0.4814  |  lr: 2.61e-04  |\n",
      "|  step: 27  |  epoch: 33  |  batch_num: 270  |  batch_dt: 50.46  |  batch_loss: 0.4741  |  lr: 2.71e-04  |\n",
      "|  step: 28  |  epoch: 35  |  batch_num: 280  |  batch_dt: 48.80  |  batch_loss: 0.4754  |  lr: 2.81e-04  |\n",
      "|  step: 29  |  epoch: 36  |  batch_num: 290  |  batch_dt: 46.57  |  batch_loss: 0.4589  |  lr: 2.91e-04  |\n",
      "|  step: 30  |  epoch: 37  |  batch_num: 300  |  batch_dt: 49.10  |  batch_loss: 0.4536  |  lr: 3.01e-04  |  train_acc: 0.7200  |  test_loss: 0.4446  |  test_acc: 0.7200  |\n",
      "|  step: 31  |  epoch: 38  |  batch_num: 310  |  batch_dt: 58.28  |  batch_loss: 0.4510  |  lr: 3.11e-04  |\n",
      "|  step: 32  |  epoch: 40  |  batch_num: 320  |  batch_dt: 47.40  |  batch_loss: 0.4355  |  lr: 3.21e-04  |\n",
      "|  step: 33  |  epoch: 41  |  batch_num: 330  |  batch_dt: 46.26  |  batch_loss: 0.4408  |  lr: 3.31e-04  |\n",
      "|  step: 34  |  epoch: 42  |  batch_num: 340  |  batch_dt: 53.27  |  batch_loss: 0.4244  |  lr: 3.41e-04  |\n",
      "|  step: 35  |  epoch: 43  |  batch_num: 350  |  batch_dt: 51.44  |  batch_loss: 0.4311  |  lr: 3.51e-04  |\n",
      "|  step: 36  |  epoch: 45  |  batch_num: 360  |  batch_dt: 51.39  |  batch_loss: 0.4217  |  lr: 3.61e-04  |\n",
      "|  step: 37  |  epoch: 46  |  batch_num: 370  |  batch_dt: 50.64  |  batch_loss: 0.4154  |  lr: 3.71e-04  |\n",
      "|  step: 38  |  epoch: 47  |  batch_num: 380  |  batch_dt: 50.88  |  batch_loss: 0.4222  |  lr: 3.81e-04  |\n",
      "|  step: 39  |  epoch: 48  |  batch_num: 390  |  batch_dt: 51.82  |  batch_loss: 0.4148  |  lr: 3.91e-04  |\n",
      "|  step: 40  |  epoch: 50  |  batch_num: 400  |  batch_dt: 48.20  |  batch_loss: 0.4232  |  lr: 4.01e-04  |  train_acc: 0.9500  |  test_loss: 0.4086  |  test_acc: 0.9400  |\n",
      "|  step: 41  |  epoch: 51  |  batch_num: 410  |  batch_dt: 49.62  |  batch_loss: 0.4101  |  lr: 4.11e-04  |\n",
      "|  step: 42  |  epoch: 52  |  batch_num: 420  |  batch_dt: 50.43  |  batch_loss: 0.4119  |  lr: 4.21e-04  |\n",
      "|  step: 43  |  epoch: 53  |  batch_num: 430  |  batch_dt: 52.80  |  batch_loss: 0.4054  |  lr: 4.31e-04  |\n",
      "|  step: 44  |  epoch: 55  |  batch_num: 440  |  batch_dt: 52.32  |  batch_loss: 0.4158  |  lr: 4.41e-04  |\n",
      "|  step: 45  |  epoch: 56  |  batch_num: 450  |  batch_dt: 51.67  |  batch_loss: 0.3997  |  lr: 4.51e-04  |\n",
      "|  step: 46  |  epoch: 57  |  batch_num: 460  |  batch_dt: 52.65  |  batch_loss: 0.3989  |  lr: 4.61e-04  |\n",
      "|  step: 47  |  epoch: 58  |  batch_num: 470  |  batch_dt: 51.39  |  batch_loss: 0.4043  |  lr: 4.71e-04  |\n",
      "|  step: 48  |  epoch: 60  |  batch_num: 480  |  batch_dt: 47.96  |  batch_loss: 0.4081  |  lr: 4.81e-04  |\n",
      "|  step: 49  |  epoch: 61  |  batch_num: 490  |  batch_dt: 48.86  |  batch_loss: 0.3981  |  lr: 4.91e-04  |\n",
      "|  step: 50  |  epoch: 62  |  batch_num: 500  |  batch_dt: 53.38  |  batch_loss: 0.4180  |  lr: 5.01e-04  |  train_acc: 0.9800  |  test_loss: 0.4077  |  test_acc: 0.9700  |\n",
      "|  step: 51  |  epoch: 63  |  batch_num: 510  |  batch_dt: 51.73  |  batch_loss: 0.4123  |  lr: 5.11e-04  |\n",
      "|  step: 52  |  epoch: 65  |  batch_num: 520  |  batch_dt: 45.69  |  batch_loss: 0.4078  |  lr: 5.21e-04  |\n",
      "|  step: 53  |  epoch: 66  |  batch_num: 530  |  batch_dt: 64.93  |  batch_loss: 0.3995  |  lr: 5.31e-04  |\n",
      "|  step: 54  |  epoch: 67  |  batch_num: 540  |  batch_dt: 46.13  |  batch_loss: 0.3974  |  lr: 5.41e-04  |\n",
      "|  step: 55  |  epoch: 68  |  batch_num: 550  |  batch_dt: 50.69  |  batch_loss: 0.3988  |  lr: 5.51e-04  |\n",
      "|  step: 56  |  epoch: 70  |  batch_num: 560  |  batch_dt: 49.53  |  batch_loss: 0.4023  |  lr: 5.61e-04  |\n",
      "|  step: 57  |  epoch: 71  |  batch_num: 570  |  batch_dt: 54.07  |  batch_loss: 0.4001  |  lr: 5.71e-04  |\n",
      "|  step: 58  |  epoch: 72  |  batch_num: 580  |  batch_dt: 58.65  |  batch_loss: 0.4059  |  lr: 5.81e-04  |\n",
      "|  step: 59  |  epoch: 73  |  batch_num: 590  |  batch_dt: 60.29  |  batch_loss: 0.3996  |  lr: 5.91e-04  |\n",
      "|  step: 60  |  epoch: 75  |  batch_num: 600  |  batch_dt: 50.23  |  batch_loss: 0.4011  |  lr: 6.01e-04  |  train_acc: 0.9800  |  test_loss: 0.3918  |  test_acc: 0.9900  |\n",
      "|  step: 61  |  epoch: 76  |  batch_num: 610  |  batch_dt: 46.79  |  batch_loss: 0.4037  |  lr: 6.11e-04  |\n",
      "|  step: 62  |  epoch: 77  |  batch_num: 620  |  batch_dt: 51.95  |  batch_loss: 0.3973  |  lr: 6.21e-04  |\n",
      "|  step: 63  |  epoch: 78  |  batch_num: 630  |  batch_dt: 50.28  |  batch_loss: 0.3945  |  lr: 6.31e-04  |\n",
      "|  step: 64  |  epoch: 80  |  batch_num: 640  |  batch_dt: 46.59  |  batch_loss: 0.3898  |  lr: 6.41e-04  |\n",
      "|  step: 65  |  epoch: 81  |  batch_num: 650  |  batch_dt: 48.14  |  batch_loss: 0.4033  |  lr: 6.51e-04  |\n",
      "|  step: 66  |  epoch: 82  |  batch_num: 660  |  batch_dt: 52.49  |  batch_loss: 0.3970  |  lr: 6.61e-04  |\n",
      "|  step: 67  |  epoch: 83  |  batch_num: 670  |  batch_dt: 49.97  |  batch_loss: 0.3993  |  lr: 6.71e-04  |\n",
      "|  step: 68  |  epoch: 85  |  batch_num: 680  |  batch_dt: 45.52  |  batch_loss: 0.3878  |  lr: 6.81e-04  |\n",
      "|  step: 69  |  epoch: 86  |  batch_num: 690  |  batch_dt: 47.35  |  batch_loss: 0.3904  |  lr: 6.91e-04  |\n",
      "|  step: 70  |  epoch: 87  |  batch_num: 700  |  batch_dt: 55.27  |  batch_loss: 0.3876  |  lr: 7.01e-04  |  train_acc: 1.0000  |  test_loss: 0.3880  |  test_acc: 1.0000  |\n",
      "|  step: 71  |  epoch: 88  |  batch_num: 710  |  batch_dt: 50.49  |  batch_loss: 0.3950  |  lr: 7.11e-04  |\n",
      "|  step: 72  |  epoch: 90  |  batch_num: 720  |  batch_dt: 46.99  |  batch_loss: 0.3926  |  lr: 7.21e-04  |\n",
      "|  step: 73  |  epoch: 91  |  batch_num: 730  |  batch_dt: 48.73  |  batch_loss: 0.3866  |  lr: 7.31e-04  |\n",
      "|  step: 74  |  epoch: 92  |  batch_num: 740  |  batch_dt: 49.84  |  batch_loss: 0.3900  |  lr: 7.41e-04  |\n",
      "|  step: 75  |  epoch: 93  |  batch_num: 750  |  batch_dt: 47.68  |  batch_loss: 0.3850  |  lr: 7.51e-04  |\n",
      "|  step: 76  |  epoch: 95  |  batch_num: 760  |  batch_dt: 48.86  |  batch_loss: 0.3849  |  lr: 7.61e-04  |\n",
      "|  step: 77  |  epoch: 96  |  batch_num: 770  |  batch_dt: 45.02  |  batch_loss: 0.3947  |  lr: 7.71e-04  |\n",
      "|  step: 78  |  epoch: 97  |  batch_num: 780  |  batch_dt: 50.53  |  batch_loss: 0.3880  |  lr: 7.81e-04  |\n",
      "|  step: 79  |  epoch: 98  |  batch_num: 790  |  batch_dt: 46.18  |  batch_loss: 0.3757  |  lr: 7.91e-04  |\n",
      "|  step: 80  |  epoch: 100  |  batch_num: 800  |  batch_dt: 51.02  |  batch_loss: 0.3809  |  lr: 8.01e-04  |  train_acc: 1.0000  |  test_loss: 0.3811  |  test_acc: 0.9900  |\n",
      "|  step: 81  |  epoch: 101  |  batch_num: 810  |  batch_dt: 46.87  |  batch_loss: 0.3855  |  lr: 8.11e-04  |\n",
      "|  step: 82  |  epoch: 102  |  batch_num: 820  |  batch_dt: 51.53  |  batch_loss: 0.3814  |  lr: 8.21e-04  |\n",
      "|  step: 83  |  epoch: 103  |  batch_num: 830  |  batch_dt: 63.31  |  batch_loss: 0.3772  |  lr: 8.31e-04  |\n",
      "|  step: 84  |  epoch: 105  |  batch_num: 840  |  batch_dt: 48.37  |  batch_loss: 0.3967  |  lr: 8.41e-04  |\n",
      "|  step: 85  |  epoch: 106  |  batch_num: 850  |  batch_dt: 48.68  |  batch_loss: 0.3796  |  lr: 8.51e-04  |\n",
      "|  step: 86  |  epoch: 107  |  batch_num: 860  |  batch_dt: 52.27  |  batch_loss: 0.3786  |  lr: 8.61e-04  |\n",
      "|  step: 87  |  epoch: 108  |  batch_num: 870  |  batch_dt: 48.92  |  batch_loss: 0.3798  |  lr: 8.71e-04  |\n",
      "|  step: 88  |  epoch: 110  |  batch_num: 880  |  batch_dt: 49.37  |  batch_loss: 0.3785  |  lr: 8.81e-04  |\n",
      "|  step: 89  |  epoch: 111  |  batch_num: 890  |  batch_dt: 44.16  |  batch_loss: 0.3753  |  lr: 8.91e-04  |\n",
      "|  step: 90  |  epoch: 112  |  batch_num: 900  |  batch_dt: 49.18  |  batch_loss: 0.3749  |  lr: 9.01e-04  |  train_acc: 1.0000  |  test_loss: 0.3822  |  test_acc: 1.0000  |\n",
      "|  step: 91  |  epoch: 113  |  batch_num: 910  |  batch_dt: 46.67  |  batch_loss: 0.3881  |  lr: 9.11e-04  |\n",
      "|  step: 92  |  epoch: 115  |  batch_num: 920  |  batch_dt: 47.24  |  batch_loss: 0.3790  |  lr: 9.21e-04  |\n",
      "|  step: 93  |  epoch: 116  |  batch_num: 930  |  batch_dt: 51.55  |  batch_loss: 0.3713  |  lr: 9.31e-04  |\n",
      "|  step: 94  |  epoch: 117  |  batch_num: 940  |  batch_dt: 47.25  |  batch_loss: 0.3792  |  lr: 9.41e-04  |\n",
      "|  step: 95  |  epoch: 118  |  batch_num: 950  |  batch_dt: 55.17  |  batch_loss: 0.3703  |  lr: 9.51e-04  |\n",
      "|  step: 96  |  epoch: 120  |  batch_num: 960  |  batch_dt: 49.10  |  batch_loss: 0.3800  |  lr: 9.61e-04  |\n",
      "|  step: 97  |  epoch: 121  |  batch_num: 970  |  batch_dt: 46.38  |  batch_loss: 0.3876  |  lr: 9.71e-04  |\n",
      "|  step: 98  |  epoch: 122  |  batch_num: 980  |  batch_dt: 45.37  |  batch_loss: 0.3800  |  lr: 9.81e-04  |\n",
      "|  step: 99  |  epoch: 123  |  batch_num: 990  |  batch_dt: 54.50  |  batch_loss: 0.3765  |  lr: 9.91e-04  |\n",
      "|  step: 100  |  epoch: 125  |  batch_num: 1000  |  batch_dt: 48.01  |  batch_loss: 0.3717  |  lr: 1.00e-03  |  train_acc: 0.9900  |  test_loss: 0.3778  |  test_acc: 1.0000  |\n",
      "|  step: 101  |  epoch: 126  |  batch_num: 1010  |  batch_dt: 48.38  |  batch_loss: 0.3777  |  lr: 9.96e-04  |\n",
      "|  step: 102  |  epoch: 127  |  batch_num: 1020  |  batch_dt: 49.46  |  batch_loss: 0.3773  |  lr: 9.93e-04  |\n",
      "|  step: 103  |  epoch: 128  |  batch_num: 1030  |  batch_dt: 53.40  |  batch_loss: 0.3757  |  lr: 9.90e-04  |\n",
      "|  step: 104  |  epoch: 130  |  batch_num: 1040  |  batch_dt: 48.26  |  batch_loss: 0.3689  |  lr: 9.86e-04  |\n",
      "|  step: 105  |  epoch: 131  |  batch_num: 1050  |  batch_dt: 48.26  |  batch_loss: 0.3721  |  lr: 9.83e-04  |\n",
      "|  step: 106  |  epoch: 132  |  batch_num: 1060  |  batch_dt: 51.71  |  batch_loss: 0.3754  |  lr: 9.80e-04  |\n",
      "|  step: 107  |  epoch: 133  |  batch_num: 1070  |  batch_dt: 51.66  |  batch_loss: 0.3746  |  lr: 9.77e-04  |\n",
      "|  step: 108  |  epoch: 135  |  batch_num: 1080  |  batch_dt: 47.78  |  batch_loss: 0.3703  |  lr: 9.73e-04  |\n",
      "|  step: 109  |  epoch: 136  |  batch_num: 1090  |  batch_dt: 47.33  |  batch_loss: 0.3702  |  lr: 9.70e-04  |\n",
      "|  step: 110  |  epoch: 137  |  batch_num: 1100  |  batch_dt: 53.75  |  batch_loss: 0.3647  |  lr: 9.67e-04  |  train_acc: 1.0000  |  test_loss: 0.3743  |  test_acc: 1.0000  |\n",
      "|  step: 111  |  epoch: 138  |  batch_num: 1110  |  batch_dt: 54.54  |  batch_loss: 0.3699  |  lr: 9.63e-04  |\n",
      "|  step: 112  |  epoch: 140  |  batch_num: 1120  |  batch_dt: 47.06  |  batch_loss: 0.3683  |  lr: 9.60e-04  |\n",
      "|  step: 113  |  epoch: 141  |  batch_num: 1130  |  batch_dt: 48.30  |  batch_loss: 0.3706  |  lr: 9.57e-04  |\n",
      "|  step: 114  |  epoch: 142  |  batch_num: 1140  |  batch_dt: 51.73  |  batch_loss: 0.3662  |  lr: 9.53e-04  |\n",
      "|  step: 115  |  epoch: 143  |  batch_num: 1150  |  batch_dt: 51.42  |  batch_loss: 0.3632  |  lr: 9.50e-04  |\n",
      "|  step: 116  |  epoch: 145  |  batch_num: 1160  |  batch_dt: 48.39  |  batch_loss: 0.3722  |  lr: 9.47e-04  |\n",
      "|  step: 117  |  epoch: 146  |  batch_num: 1170  |  batch_dt: 48.39  |  batch_loss: 0.3589  |  lr: 9.44e-04  |\n",
      "|  step: 118  |  epoch: 147  |  batch_num: 1180  |  batch_dt: 54.96  |  batch_loss: 0.3613  |  lr: 9.40e-04  |\n",
      "|  step: 119  |  epoch: 148  |  batch_num: 1190  |  batch_dt: 49.79  |  batch_loss: 0.3691  |  lr: 9.37e-04  |\n",
      "|  step: 120  |  epoch: 150  |  batch_num: 1200  |  batch_dt: 48.59  |  batch_loss: 0.3645  |  lr: 9.34e-04  |  train_acc: 0.9900  |  test_loss: 0.3704  |  test_acc: 1.0000  |\n",
      "|  step: 121  |  epoch: 151  |  batch_num: 1210  |  batch_dt: 48.21  |  batch_loss: 0.3605  |  lr: 9.30e-04  |\n",
      "|  step: 122  |  epoch: 152  |  batch_num: 1220  |  batch_dt: 51.99  |  batch_loss: 0.3683  |  lr: 9.27e-04  |\n",
      "|  step: 123  |  epoch: 153  |  batch_num: 1230  |  batch_dt: 49.01  |  batch_loss: 0.3661  |  lr: 9.24e-04  |\n",
      "|  step: 124  |  epoch: 155  |  batch_num: 1240  |  batch_dt: 48.04  |  batch_loss: 0.3634  |  lr: 9.20e-04  |\n",
      "|  step: 125  |  epoch: 156  |  batch_num: 1250  |  batch_dt: 48.06  |  batch_loss: 0.3613  |  lr: 9.17e-04  |\n",
      "|  step: 126  |  epoch: 157  |  batch_num: 1260  |  batch_dt: 49.90  |  batch_loss: 0.3602  |  lr: 9.14e-04  |\n",
      "|  step: 127  |  epoch: 158  |  batch_num: 1270  |  batch_dt: 47.90  |  batch_loss: 0.3795  |  lr: 9.11e-04  |\n",
      "|  step: 128  |  epoch: 160  |  batch_num: 1280  |  batch_dt: 46.80  |  batch_loss: 0.3647  |  lr: 9.07e-04  |\n",
      "|  step: 129  |  epoch: 161  |  batch_num: 1290  |  batch_dt: 48.02  |  batch_loss: 0.3547  |  lr: 9.04e-04  |\n",
      "|  step: 130  |  epoch: 162  |  batch_num: 1300  |  batch_dt: 50.67  |  batch_loss: 0.3624  |  lr: 9.01e-04  |  train_acc: 1.0000  |  test_loss: 0.3801  |  test_acc: 1.0000  |\n",
      "|  step: 131  |  epoch: 163  |  batch_num: 1310  |  batch_dt: 49.52  |  batch_loss: 0.3667  |  lr: 8.97e-04  |\n",
      "|  step: 132  |  epoch: 165  |  batch_num: 1320  |  batch_dt: 44.98  |  batch_loss: 0.3562  |  lr: 8.94e-04  |\n",
      "|  step: 133  |  epoch: 166  |  batch_num: 1330  |  batch_dt: 50.67  |  batch_loss: 0.3569  |  lr: 8.91e-04  |\n",
      "|  step: 134  |  epoch: 167  |  batch_num: 1340  |  batch_dt: 50.99  |  batch_loss: 0.3610  |  lr: 8.87e-04  |\n",
      "|  step: 135  |  epoch: 168  |  batch_num: 1350  |  batch_dt: 55.67  |  batch_loss: 0.3631  |  lr: 8.84e-04  |\n",
      "|  step: 136  |  epoch: 170  |  batch_num: 1360  |  batch_dt: 50.01  |  batch_loss: 0.3581  |  lr: 8.81e-04  |\n",
      "|  step: 137  |  epoch: 171  |  batch_num: 1370  |  batch_dt: 49.07  |  batch_loss: 0.3598  |  lr: 8.78e-04  |\n",
      "|  step: 138  |  epoch: 172  |  batch_num: 1380  |  batch_dt: 51.23  |  batch_loss: 0.3538  |  lr: 8.74e-04  |\n",
      "|  step: 139  |  epoch: 173  |  batch_num: 1390  |  batch_dt: 51.25  |  batch_loss: 0.3545  |  lr: 8.71e-04  |\n",
      "|  step: 140  |  epoch: 175  |  batch_num: 1400  |  batch_dt: 51.94  |  batch_loss: 0.3579  |  lr: 8.68e-04  |  train_acc: 0.9900  |  test_loss: 0.3657  |  test_acc: 1.0000  |\n",
      "|  step: 141  |  epoch: 176  |  batch_num: 1410  |  batch_dt: 49.28  |  batch_loss: 0.3690  |  lr: 8.64e-04  |\n",
      "|  step: 142  |  epoch: 177  |  batch_num: 1420  |  batch_dt: 51.60  |  batch_loss: 0.3620  |  lr: 8.61e-04  |\n",
      "|  step: 143  |  epoch: 178  |  batch_num: 1430  |  batch_dt: 53.17  |  batch_loss: 0.3561  |  lr: 8.58e-04  |\n",
      "|  step: 144  |  epoch: 180  |  batch_num: 1440  |  batch_dt: 54.17  |  batch_loss: 0.3619  |  lr: 8.54e-04  |\n",
      "|  step: 145  |  epoch: 181  |  batch_num: 1450  |  batch_dt: 51.82  |  batch_loss: 0.3620  |  lr: 8.51e-04  |\n",
      "|  step: 146  |  epoch: 182  |  batch_num: 1460  |  batch_dt: 50.54  |  batch_loss: 0.3564  |  lr: 8.48e-04  |\n",
      "|  step: 147  |  epoch: 183  |  batch_num: 1470  |  batch_dt: 52.23  |  batch_loss: 0.3581  |  lr: 8.45e-04  |\n",
      "|  step: 148  |  epoch: 185  |  batch_num: 1480  |  batch_dt: 48.83  |  batch_loss: 0.3563  |  lr: 8.41e-04  |\n",
      "|  step: 149  |  epoch: 186  |  batch_num: 1490  |  batch_dt: 47.87  |  batch_loss: 0.3555  |  lr: 8.38e-04  |\n",
      "|  step: 150  |  epoch: 187  |  batch_num: 1500  |  batch_dt: 50.77  |  batch_loss: 0.3590  |  lr: 8.35e-04  |  train_acc: 1.0000  |  test_loss: 0.3724  |  test_acc: 0.9900  |\n",
      "|  step: 151  |  epoch: 188  |  batch_num: 1510  |  batch_dt: 49.29  |  batch_loss: 0.3549  |  lr: 8.31e-04  |\n",
      "|  step: 152  |  epoch: 190  |  batch_num: 1520  |  batch_dt: 55.87  |  batch_loss: 0.3496  |  lr: 8.28e-04  |\n",
      "|  step: 153  |  epoch: 191  |  batch_num: 1530  |  batch_dt: 49.69  |  batch_loss: 0.3527  |  lr: 8.25e-04  |\n",
      "|  step: 154  |  epoch: 192  |  batch_num: 1540  |  batch_dt: 76.65  |  batch_loss: 0.3551  |  lr: 8.21e-04  |\n",
      "|  step: 155  |  epoch: 193  |  batch_num: 1550  |  batch_dt: 58.15  |  batch_loss: 0.3516  |  lr: 8.18e-04  |\n",
      "|  step: 156  |  epoch: 195  |  batch_num: 1560  |  batch_dt: 49.76  |  batch_loss: 0.3508  |  lr: 8.15e-04  |\n",
      "|  step: 157  |  epoch: 196  |  batch_num: 1570  |  batch_dt: 48.78  |  batch_loss: 0.3535  |  lr: 8.12e-04  |\n",
      "|  step: 158  |  epoch: 197  |  batch_num: 1580  |  batch_dt: 52.76  |  batch_loss: 0.3554  |  lr: 8.08e-04  |\n",
      "|  step: 159  |  epoch: 198  |  batch_num: 1590  |  batch_dt: 55.27  |  batch_loss: 0.3455  |  lr: 8.05e-04  |\n",
      "|  step: 160  |  epoch: 200  |  batch_num: 1600  |  batch_dt: 54.74  |  batch_loss: 0.3488  |  lr: 8.02e-04  |  train_acc: 1.0000  |  test_loss: 0.3751  |  test_acc: 0.9900  |\n",
      "|  step: 161  |  epoch: 201  |  batch_num: 1610  |  batch_dt: 51.68  |  batch_loss: 0.3560  |  lr: 7.98e-04  |\n",
      "|  step: 162  |  epoch: 202  |  batch_num: 1620  |  batch_dt: 53.38  |  batch_loss: 0.3497  |  lr: 7.95e-04  |\n",
      "|  step: 163  |  epoch: 203  |  batch_num: 1630  |  batch_dt: 50.96  |  batch_loss: 0.3506  |  lr: 7.92e-04  |\n",
      "|  step: 164  |  epoch: 205  |  batch_num: 1640  |  batch_dt: 51.82  |  batch_loss: 0.3445  |  lr: 7.88e-04  |\n",
      "|  step: 165  |  epoch: 206  |  batch_num: 1650  |  batch_dt: 65.04  |  batch_loss: 0.3539  |  lr: 7.85e-04  |\n",
      "|  step: 166  |  epoch: 207  |  batch_num: 1660  |  batch_dt: 50.86  |  batch_loss: 0.3581  |  lr: 7.82e-04  |\n",
      "|  step: 167  |  epoch: 208  |  batch_num: 1670  |  batch_dt: 53.41  |  batch_loss: 0.3526  |  lr: 7.79e-04  |\n",
      "|  step: 168  |  epoch: 210  |  batch_num: 1680  |  batch_dt: 53.76  |  batch_loss: 0.3478  |  lr: 7.75e-04  |\n",
      "|  step: 169  |  epoch: 211  |  batch_num: 1690  |  batch_dt: 50.85  |  batch_loss: 0.3469  |  lr: 7.72e-04  |\n",
      "|  step: 170  |  epoch: 212  |  batch_num: 1700  |  batch_dt: 50.89  |  batch_loss: 0.3481  |  lr: 7.69e-04  |  train_acc: 1.0000  |  test_loss: 0.3673  |  test_acc: 1.0000  |\n",
      "|  step: 171  |  epoch: 213  |  batch_num: 1710  |  batch_dt: 50.90  |  batch_loss: 0.3491  |  lr: 7.65e-04  |\n",
      "|  step: 172  |  epoch: 215  |  batch_num: 1720  |  batch_dt: 48.99  |  batch_loss: 0.3482  |  lr: 7.62e-04  |\n",
      "|  step: 173  |  epoch: 216  |  batch_num: 1730  |  batch_dt: 60.79  |  batch_loss: 0.3515  |  lr: 7.59e-04  |\n",
      "|  step: 174  |  epoch: 217  |  batch_num: 1740  |  batch_dt: 50.53  |  batch_loss: 0.3475  |  lr: 7.55e-04  |\n",
      "|  step: 175  |  epoch: 218  |  batch_num: 1750  |  batch_dt: 52.45  |  batch_loss: 0.3511  |  lr: 7.52e-04  |\n",
      "|  step: 176  |  epoch: 220  |  batch_num: 1760  |  batch_dt: 53.39  |  batch_loss: 0.3496  |  lr: 7.49e-04  |\n",
      "|  step: 177  |  epoch: 221  |  batch_num: 1770  |  batch_dt: 63.29  |  batch_loss: 0.3432  |  lr: 7.46e-04  |\n",
      "|  step: 178  |  epoch: 222  |  batch_num: 1780  |  batch_dt: 50.42  |  batch_loss: 0.3455  |  lr: 7.42e-04  |\n",
      "|  step: 179  |  epoch: 223  |  batch_num: 1790  |  batch_dt: 51.80  |  batch_loss: 0.3570  |  lr: 7.39e-04  |\n",
      "|  step: 180  |  epoch: 225  |  batch_num: 1800  |  batch_dt: 52.45  |  batch_loss: 0.3476  |  lr: 7.36e-04  |  train_acc: 1.0000  |  test_loss: 0.3690  |  test_acc: 0.9900  |\n",
      "|  step: 181  |  epoch: 226  |  batch_num: 1810  |  batch_dt: 49.15  |  batch_loss: 0.3482  |  lr: 7.32e-04  |\n",
      "|  step: 182  |  epoch: 227  |  batch_num: 1820  |  batch_dt: 49.73  |  batch_loss: 0.3496  |  lr: 7.29e-04  |\n",
      "|  step: 183  |  epoch: 228  |  batch_num: 1830  |  batch_dt: 52.94  |  batch_loss: 0.3416  |  lr: 7.26e-04  |\n",
      "|  step: 184  |  epoch: 230  |  batch_num: 1840  |  batch_dt: 51.99  |  batch_loss: 0.3509  |  lr: 7.22e-04  |\n",
      "|  step: 185  |  epoch: 231  |  batch_num: 1850  |  batch_dt: 51.92  |  batch_loss: 0.3515  |  lr: 7.19e-04  |\n",
      "|  step: 186  |  epoch: 232  |  batch_num: 1860  |  batch_dt: 54.12  |  batch_loss: 0.3457  |  lr: 7.16e-04  |\n",
      "|  step: 187  |  epoch: 233  |  batch_num: 1870  |  batch_dt: 51.07  |  batch_loss: 0.3451  |  lr: 7.13e-04  |\n",
      "|  step: 188  |  epoch: 235  |  batch_num: 1880  |  batch_dt: 50.64  |  batch_loss: 0.3506  |  lr: 7.09e-04  |\n",
      "|  step: 189  |  epoch: 236  |  batch_num: 1890  |  batch_dt: 50.54  |  batch_loss: 0.3517  |  lr: 7.06e-04  |\n",
      "|  step: 190  |  epoch: 237  |  batch_num: 1900  |  batch_dt: 50.84  |  batch_loss: 0.3413  |  lr: 7.03e-04  |  train_acc: 1.0000  |  test_loss: 0.3734  |  test_acc: 0.9900  |\n",
      "|  step: 191  |  epoch: 238  |  batch_num: 1910  |  batch_dt: 49.68  |  batch_loss: 0.3485  |  lr: 6.99e-04  |\n",
      "|  step: 192  |  epoch: 240  |  batch_num: 1920  |  batch_dt: 47.95  |  batch_loss: 0.3519  |  lr: 6.96e-04  |\n",
      "|  step: 193  |  epoch: 241  |  batch_num: 1930  |  batch_dt: 45.81  |  batch_loss: 0.3445  |  lr: 6.93e-04  |\n",
      "|  step: 194  |  epoch: 242  |  batch_num: 1940  |  batch_dt: 58.86  |  batch_loss: 0.3508  |  lr: 6.89e-04  |\n",
      "|  step: 195  |  epoch: 243  |  batch_num: 1950  |  batch_dt: 49.70  |  batch_loss: 0.3493  |  lr: 6.86e-04  |\n",
      "|  step: 196  |  epoch: 245  |  batch_num: 1960  |  batch_dt: 48.71  |  batch_loss: 0.3404  |  lr: 6.83e-04  |\n",
      "|  step: 197  |  epoch: 246  |  batch_num: 1970  |  batch_dt: 48.02  |  batch_loss: 0.3498  |  lr: 6.80e-04  |\n",
      "|  step: 198  |  epoch: 247  |  batch_num: 1980  |  batch_dt: 64.42  |  batch_loss: 0.3401  |  lr: 6.76e-04  |\n",
      "|  step: 199  |  epoch: 248  |  batch_num: 1990  |  batch_dt: 50.50  |  batch_loss: 0.3443  |  lr: 6.73e-04  |\n"
     ]
    }
   ],
   "source": [
    "from origami.inference import Predictor\n",
    "from origami.utils import make_progress_callback\n",
    "\n",
    "# train config\n",
    "config.train.learning_rate = 1e-3\n",
    "config.train.print_every = 10\n",
    "config.train.eval_every = 100\n",
    "\n",
    "# create a predictor\n",
    "predictor = Predictor(model, encoder, target_field)\n",
    "\n",
    "# create and register progress callback\n",
    "progress_callback = make_progress_callback(\n",
    "    config.train, train_dataset=train_dataset, test_dataset=test_dataset, predictor=predictor\n",
    ")\n",
    "model.set_callback(\"on_batch_end\", progress_callback)\n",
    "\n",
    "# train model\n",
    "model.train_model(train_dataset, batches=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Finally, we evaluate the model on the full test set, and compare the first predictions to the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d22587741cc4b038baa6a58895cf061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.0000\n",
      "Model predictions (first 10):  ['positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative']\n",
      "Correct labels (first 10):  ['positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# calculate test accuracy\n",
    "acc = predictor.accuracy(test_dataset, show_progress=True)\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "# we can also access the predictions with the `predict()` method\n",
    "predictions = predictor.predict(test_dataset)\n",
    "print(\"Model predictions (first 10): \", predictions[:10])\n",
    "print(\"Correct labels (first 10): \", test_dataset.df[\"target\"].to_list()[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
